import re
import random
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import math
from persona import FALLBACK_RESPONSES, SENTIMENT_RESPONSES, COMPLEX_MARKERS, SEARCH_MARKERS

class SmartLocalAI:
    """–£–º–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è AI —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –∏ –ø–µ—Ä—Å–æ–Ω–æ–π"""

    def __init__(self, knowledge_manager):
        self.km = knowledge_manager
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (topic tracking)
        # {user_id: {"topic": "unknown", "last_intent": "greeting", "timestamp": ...}}
        self.conversation_states = {}

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–∏–ø–∞–º
        self.question_patterns = {
            'who': r'\b(–∫—Ç–æ|–∫–æ–≥–æ|–∫–æ–º—É|–∫–µ–º|—á–µ–π|—á—å—è)\b',
            'what': r'\b(—á—Ç–æ|—á–µ–≥–æ|—á–µ–º—É|—á–µ–º|–∫–∞–∫–æ–π|–∫–∞–∫–∞—è|–∫–∞–∫–æ–µ|–∫–∞–∫–∏–µ)\b',
            'where': r'\b(–≥–¥–µ|–∫—É–¥–∞|–æ—Ç–∫—É–¥–∞|–≤ –∫–∞–∫–æ–º|–≤ –∫–∞–∫–æ–π)\b',
            'when': r'\b(–∫–æ–≥–¥–∞|–≤ –∫–∞–∫–æ–µ –≤—Ä–µ–º—è|–≤–æ —Å–∫–æ–ª—å–∫–æ)\b',
            'why': r'\b(–ø–æ—á–µ–º—É|–∑–∞—á–µ–º|–æ—Ç—á–µ–≥–æ|–∏–∑-–∑–∞ —á–µ–≥–æ)\b',
            'how': r'\b(–∫–∞–∫|–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º|–∫–∞–∫–∏–º —Å–ø–æ—Å–æ–±–æ–º)\b',
            'yes_no': r'\b(—ç—Ç–æ|–µ—Å—Ç—å|—è–≤–ª—è–µ—Ç—Å—è|–±—É–¥–µ—Ç|–º–æ–∂–µ—Ç|–º–æ–∂–µ—à—å|—Ç—ã)\s.*\?'
        }

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤
        self.extraction_patterns = {
            'name': [
                # –¢–æ–ª—å–∫–æ —è–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã –∏–ª–∏ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                r'^(?:–º–µ–Ω—è –∑–æ–≤—É—Ç|–∑–æ–≤–∏\s+–º–µ–Ω—è)\s+([–ê-–Ø][–∞-—è—ë]*)',
                r'^(?:my name is|call me)\s+([A-Z][a-z]+)',
                # –ë–µ–∑ "—è" —Ç.–∫. —ç—Ç–æ –¥–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
            ],
            'age': [
                r'(?:–º–Ω–µ|—è)\s+(\d+)\s+(?:–≥–æ–¥|–ª–µ—Ç)',
                r'(?:age|aged?)\s*(?:is|:)?\s*(\d+)',
                r'(\d+)\s*(?:–ª–µ—Ç|–≥–æ–¥)(?:\s+—Å—Ç–∞—Ä–æ–≥–æ)?'
            ],
            'city': [
                r'(?:—è –∏–∑|–∂–∏–≤—É –≤|–≥–æ—Ä–æ–π|–≥–æ—Ä–æ–¥)\s+([–ê-–Ø–∞-—èA-Za-z\s]+)',
                r'(?:i am from|i live in)\s+([A-Za-z\s]+)'
            ],
            'work': [
                r'(?:—Ä–∞–±–æ—Ç–∞—é|–ø—Ä–æ—Ñ–µ—Å—Å–∏—è|—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å)\s+(?:–∫–∞–∫\s+)?([–ê-–Ø–∞-—èA-Za-z\s]+)',
                r'(?:i work as|i am a)\s+([A-Za-z\s]+)'
            ],
            'likes': [
                r'(?:–ª—é–±–ª—é|–æ–±–æ–∂–∞—é|–Ω—Ä–∞–≤–∏—Ç—Å—è)\s+([–ê-–Ø–∞-—èA-Za-z\s]+)',
                r'(?:i love|i like|my favorite)\s+([A-Za-z\s]+)'
            ]
        }

        # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
        self.math_pattern = re.compile(r'(\d+(?:\.\d+)?)\s*([\+\-\*\/x√∑])\s*(\d+(?:\.\d+)?)')

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ)
        self.sentiment_patterns = {
            'joy': [r'–∫—Ä—É—Ç', r'—Å—É–ø–µ—Ä', r'–∫–ª–∞—Å—Å', r'—Ä–∞–¥–æ—Å—Ç', r'—Å—á–∞—Å—Ç–ª–∏–≤', r'–∫–∞–π—Ñ', r'–æ–≥–æ–Ω—å', r'—É—Å–ø–µ—Ö', r'–ø–æ–ª—É—á–∏–ª–æ—Å—å'],
            'sadness': [r'–≥—Ä—É—Å—Ç', r'–ø–µ—á–∞–ª—å', r'–∂–∞–ª—å', r'–ø–ª–æ—Ö–æ', r'–±–µ–¥–∞', r'—É–Ω—ã–ª', r'–æ–±–∏–¥–Ω–æ', r'—Å–ª–µ–∑'],
            'anger': [r'–±–µ—Å–∏—Ç', r'–∑–ª–æ', r'–Ω–µ–Ω–∞–≤–∏–∂—É', r'—á–µ—Ä—Ç', r'–±–ª–∏–Ω', r'—Ä–∞–∑–¥—Ä–∞–∂', r'—É–∂–∞—Å', r'–æ—Ç—Å—Ç–æ–π']
        }

        self.stop_words = {
            '—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '—É–∂–µ', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã—Å—Ç—Ä–æ', '–ø–ª–æ—Ö–æ', '—Ö–æ—Ä–æ—à–æ', 
            '–Ω–æ—Ä–º–∞–ª—å–Ω–æ', '—á–µ—Å—Ç–Ω–æ', '–ø—Ä–æ—Å—Ç–æ', '–±–ª–∏–Ω', '—É—Å—Ç–∞–ª', '—Ö–æ—á—É', '–¥–µ–ª–∞—é', '—Ç–æ–∂–µ',
            '–∏–∑', '–≤', '–Ω–∞', '—Å', '—É', '–∫', '–æ—Ç', '–¥–æ', '—á–µ—Ä–µ–∑', '–æ–∫–æ–ª–æ', '—Ä–∞–±–æ—Ç–∞—é', 
            '–∂–∏–≤—É', '–ª—é–±–ª—é', '–ø–æ—à–µ–ª', '–∏–¥—É', '–≤—á–µ—Ä–∞', '—Å–µ–≥–æ–¥–Ω—è', '—Å–µ–π—á–∞—Å', '–∑–¥–µ—Å—å', '—Ç—É—Ç'
        }

    def detect_persona_change(self, text: str) -> Tuple[Optional[str], bool]:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Ö–æ—á–µ—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–º–µ–Ω–∏—Ç—å –ª–∏—á–Ω–æ—Å—Ç—å –±–æ—Ç–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (–æ–ø–∏—Å–∞–Ω–∏–µ_–ª–∏—á–Ω–æ—Å—Ç–∏, —ç—Ç–æ_—Å–±—Ä–æ—Å)
        """
        text = text.lower().strip()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–±—Ä–æ—Å–∞
        reset_patterns = [
            r'–≤–µ—Ä–Ω–∏(?:—Å—å)?\s+(?:–≤\s+)?(?:–Ω–æ—Ä–º|–æ–±—ã—á–Ω|—Å—Ç–∞–Ω–¥–∞—Ä—Ç|–±–∞–∑–æ–≤)',
            r'—Å—Ç–∞–Ω—å\s+(?:—Å–æ–±–æ–π|–æ–±—ã—á–Ω—ã–º|–∫–∞–∫\s+—Ä–∞–Ω—å—à–µ)',
            r'—Ö–≤–∞—Ç–∏—Ç\s+–ø—Ä–∏—Ç–≤–æ—Ä—è—Ç—å—Å—è'
        ]
        
        for p in reset_patterns:
            if re.search(p, text):
                return None, True
                
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–º–µ–Ω—ã
        switch_patterns = [
            r'–±—É–¥—å\s+([–∞-—è–ê-–Ø—ë–Å\s]+)',
            r'—Å—Ç–∞–Ω—å\s+([–∞-—è–ê-–Ø—ë–Å\s]+)',
            r'–æ—Ç–≤–µ—á–∞–π\s+–∫–∞–∫\s+([–∞-—è–ê-–Ø—ë–Å\s]+)',
            r'–≥–æ–≤–æ—Ä–∏\s+–∫–∞–∫\s+([–∞-—è–ê-–Ø—ë–Å\s]+)',
            r'—Ç–µ–ø–µ—Ä—å\s+—Ç—ã\s+([–∞-—è–ê-–Ø—ë–Å\s]+)'
        ]
        
        for p in switch_patterns:
            match = re.search(p, text)
            if match:
                persona_desc = match.group(1).strip()
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ —á–∏—Å—Ç–æ –ø—É—Å—Ç—ã–µ
                if len(persona_desc) > 2:
                    return persona_desc, False
                    
        return None, False

    def tokenize(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        tokens = text.split()
        return [t for t in tokens if len(t) > 2]

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF"""
        tokens1 = self.tokenize(text1)
        tokens2 = self.tokenize(text2)

        if not tokens1 or not tokens2:
            return 0.0

        all_tokens = list(set(tokens1 + tokens2))
        tf1 = {t: tokens1.count(t) / len(tokens1) for t in all_tokens}
        tf2 = {t: tokens2.count(t) / len(tokens2) for t in all_tokens}

        dot_product = sum(tf1[t] * tf2[t] for t in all_tokens)
        norm1 = math.sqrt(sum(v**2 for v in tf1.values()))
        norm2 = math.sqrt(sum(v**2 for v in tf2.values()))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def classify_question(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞"""
        text_lower = text.lower()
        for q_type, pattern in self.question_patterns.items():
            if re.search(pattern, text_lower, re.IGNORECASE):
                return q_type
        return 'general'

    def extract_entities(self, text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        entities = {}

        # –¢–æ–ª—å–∫–æ –∏—â–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —è–≤–Ω–æ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –±–æ—Ç—É
        # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–º–µ–Ω –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        text_start = text[:50].lower()
        is_direct_address = any(
            text_start.startswith(phrase)
            for phrase in ['–º–µ–Ω—è –∑–æ–≤—É—Ç', '–∑–æ–≤–∏ –º–µ–Ω—è', 'my name is', 'call me', '—è –∏–∑', '–∂–∏–≤—É –≤', '—Ä–∞–±–æ—Ç–∞—é', '–º–Ω–µ']
        )

        for entity_type, patterns in self.extraction_patterns.items():
            # –î–ª—è –∏–º–µ–Ω - —Ç—Ä–µ–±—É–µ–º —è–≤–Ω–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ
            if entity_type == 'name' and not is_direct_address:
                continue

            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
                if match:
                    value = match.group(1).strip()
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –¥–ª–∏–Ω—É –¥–ª—è –∏–º–µ–Ω
                    if entity_type == 'name':
                        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                        if value.lower() in self.stop_words or len(value) < 2:
                            continue
                        # –ò–º—è –¥–æ–ª–∂–Ω–æ –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã
                        if not value[0].isupper():
                            continue
                    if value:
                        entities[entity_type] = value
                        break
        return entities

    def detect_sentiment(self, text: str) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —Å–æ–æ–±—â–µ–Ω–∏—è"""
        text_lower = text.lower()
        for sentiment, patterns in self.sentiment_patterns.items():
            if any(re.search(p, text_lower) for p in patterns):
                return sentiment
        return None
    def track_topic(self, user_id: int, text: str, intent: Optional[str] = None):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ç–µ–º—É —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        if user_id not in self.conversation_states:
            self.conversation_states[user_id] = {
                "topics": [],
                "last_intent": "none",
                "timestamp": datetime.now(),
                "last_greeting_time": None
            }
        
        state = self.conversation_states[user_id]
        if intent:
            state["last_intent"] = str(intent)
            if intent == 'greeting':
                state["last_greeting_time"] = datetime.now()
        state["timestamp"] = datetime.now()
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ —Ç–µ–º—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        tokens = self.tokenize(text)
        if tokens:
            # –ë–µ—Ä–µ–º —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Ç–µ–º—É
            potential_topic = max(tokens, key=len)
            topics = state.get("topics")
            if isinstance(topics, list):
                if potential_topic not in topics:
                    topics.append(potential_topic)
                    if len(topics) > 5:
                        topics.pop(0)

    def solve_math(self, text: str) -> Optional[str]:
        """–†–µ—à–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É"""
        match = self.math_pattern.search(text)
        if match:
            try:
                n1 = match.group(1)
                op = match.group(2)
                n2 = match.group(3)
                if not n1 or not op or not n2:
                    return None
                
                num1 = float(n1)
                num2 = float(n2)

                if op == '+':
                    result = num1 + num2
                elif op == '-':
                    result = num1 - num2
                elif op in ['*', 'x', '¬∑']:
                    result = num1 * num2
                elif op in ['/', '√∑']:
                    if num2 != 0:
                        result = num1 / num2
                        if result == int(result):
                            result = int(result)
                    else:
                        return "–î–µ–ª–∏—Ç—å –Ω–∞ –Ω–æ–ª—å –Ω–µ–ª—å–∑—è, –¥–∞–∂–µ –º–Ω–µ! üö´"
                else:
                    return None

                if isinstance(result, float) and result != int(result):
                    return f"–≠—Ç–æ –ª–µ–≥–∫–æ: {num1:g} {op} {num2:g} = {result:g} ü§ì"
                else:
                    return f"–ò–∑–∏: {int(num1)} {op} {int(num2)} = {int(result)} üòé"

            except Exception as e:
                return None
        return None

    def find_relevant_responses(self, query: str, user_id: int, limit: int = 5) -> List[Dict]:
        """–ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏"""
        relevant = []
        all_facts = self.km.get_all_facts()

        for key, fact_list in all_facts.items():
            for fact in fact_list:
                if fact.get('added_by') == user_id:
                    continue

                message_text = fact['fact']
                similarity = self.calculate_similarity(query, message_text)

                if similarity > 0.2:
                    relevant.append({
                        'text': message_text,
                        'similarity': similarity,
                        'user': fact.get('username', 'Unknown'),
                        'timestamp': fact.get('timestamp', '')
                    })

        relevant.sort(key=lambda x: x['similarity'], reverse=True)
        return relevant[:limit]

    def detect_conversational_intent(self, text: str) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        text_lower = text.lower().strip()
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        clean_text = re.sub(r'[^\w\s]', '', text_lower)
        tokens = set(clean_text.split())

        # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±—Ä–∞–Ω—ã –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é –¥—Ä—É–≥–∏—Ö —Å–ª–æ–≤
        first_word = clean_text.split()[0] if clean_text.split() else ""
        greeting_words = ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '—Ö–∞–π', '—Ö–∞—é', '–¥–∞—Ä–æ–≤–∞', '—Å–∞–ª—é—Ç', '–π–æ—É', '–∑–¥–∞—Ä–æ–≤–∞']

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏:
        # 1. –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ - –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
        # 2. –ò–õ–ò —ç—Ç–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏ –æ–Ω–æ —Ç–æ—á–Ω–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ (–Ω–µ —á–∞—Å—Ç—å –¥—Ä—É–≥–æ–≥–æ —Å–ª–æ–≤–∞)
        if first_word in greeting_words:
            return 'greeting'

        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if len(tokens) == 1:
            short_greetings = ['–ø—Ä–∏–≤–µ—Ç', '—Ö–∞–π', '—Å–∞–ª—é—Ç', '–π–æ—É', '–¥–∞—Ä–æ–≤–∞', '–∑–¥–∞—Ä–æ–≤–∞']
            if any(w == token for token in tokens for w in short_greetings):
                return 'greeting'
        
        # –ü—Ä–æ—â–∞–Ω–∏—è
        if any(w in tokens for w in ['–ø–æ–∫–∞', '–ø—Ä–æ—â–∞–π', '–±–±', '—Å–ª–∞–¥–∫–∏—Ö', '—Å–Ω–æ–≤–∏–¥–µ–Ω–∏–π']):
            return 'farewell'

        # –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å
        if any(w in tokens for w in ['—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä—é', '—Å–ø—Å', 'thx', '–æ—Ç –¥—É—à–∏', '–ø–∞—Å–∏–±']):
            return 'gratitude'

        # –ö–∞–∫ –¥–µ–ª–∞
        if any(phrase in clean_text for phrase in ['–∫–∞–∫ –¥–µ–ª–∞', '–∫–∞–∫ —Ç—ã', '–∫–∞–∫ —Å–∞–º', '—á–µ –∫–∞–∫', '–∫–∞–∫ –∂–∏–∑–Ω—å']):
            return 'how_are_you'

        # –í–æ–ø—Ä–æ—Å –æ —Å–æ–∑–¥–∞—Ç–µ–ª–µ
        if any(phrase in clean_text for phrase in ['–∫—Ç–æ —Ç–≤–æ–π —Å–æ–∑–¥–∞—Ç–µ–ª—å', '–∫—Ç–æ —Ç–µ–±—è —Å–æ–∑–¥–∞–ª', '—á–µ–π —Ç—ã', '–∫—Ç–æ –∞–≤—Ç–æ—Ä']):
            return 'creator'

        # –ß—Ç–æ —Ç—ã —É–º–µ–µ—à—å
        if any(phrase in clean_text for phrase in ['—á—Ç–æ —É–º–µ–µ—à—å', '—á—Ç–æ —Ç—ã –º–æ–∂–µ—à—å', '—Ç–≤–æ–∏ —Ñ—É–Ω–∫—Ü–∏–∏', '—á–µ —É–º–µ–µ—à—å']):
            return 'capabilities'

        # –®—É—Ç–∫–∞
        if any(w in tokens for w in ['—à—É—Ç–∫–∞', '–∞–Ω–µ–∫–¥–æ—Ç', '–ø–æ—à—É—Ç–∏', '—Ä–∞—Å—Å–º–µ—à–∏', '–ø—Ä–∏–∫–æ–ª']):
            return 'joke'

        # –°–∫—É–∫–∞
        if any(phrase in clean_text for phrase in ['—Å–∫—É—á–Ω–æ', '–º–Ω–µ —Å–∫—É—á–Ω–æ', '–Ω–µ—á–µ–º –∑–∞–Ω—è—Ç—å—Å—è']):
            return 'boredom'

        # –ö—Ç–æ —Ç—ã
        if any(phrase in clean_text for phrase in ['–∫—Ç–æ —Ç—ã', '—Ç—ã –∫—Ç–æ', '–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç', '—Ç–≤–æ–µ –∏–º—è']):
            if '–∫—Ç–æ —è' not in clean_text:
                return 'bot_identity'
            
        return None

    def get_user_contextual_response(self, user_id: int, intent: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è FALLBACK_RESPONSES"""
        user_name = self.km.get_user_name(user_id) or "–¥—Ä—É–≥"
        
        # –ï—Å–ª–∏ —Ç–∞–∫–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞ –Ω–µ—Ç –≤ –Ω–∞—à–∏—Ö —à–∞–±–ª–æ–Ω–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, farewell –Ω–µ—Ç –≤ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º), –¥–æ–±–∞–≤–∏–º –¥–µ—Ñ–æ–ª—Ç
        responses = FALLBACK_RESPONSES.get(intent)
        
        if not responses:
            return None

        response = random.choice(responses)
        return response.format(name=user_name)

    def generate_smart_response(self, message: str, user_id: int, username: str = None) -> Tuple[str, float]:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–º–Ω—ã–π –æ—Ç–≤–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
        Returns: (–æ—Ç–≤–µ—Ç, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
        """
        message_lower = message.lower().strip()

        # 1. –°–µ–Ω—Ç–∏–º–µ–Ω—Ç/–≠–º–æ—Ü–∏–∏ (–≤—Å–µ–≥–¥–∞ –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ)
        sentiment = self.detect_sentiment(message)
        if sentiment:
            user_name = self.km.get_user_name(user_id) or "–¥—Ä—É–≥"
            responses = SENTIMENT_RESPONSES.get(sentiment)
            if responses:
                # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–µ (—ç–º–æ—Ü–∏—è), —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã—à–µ
                confidence = 0.85 if len(message_lower.split()) < 4 else 0.4
                res = random.choice(responses).format(name=user_name)
                # –î–æ–±–∞–≤–ª—è–µ–º "—á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏" (–æ–ø–µ—á–∞—Ç–∫–∏ –∏–ª–∏ –∫–∞–∑—É–∞–ª—å–Ω–æ—Å—Ç—å)
                if random.random() < 0.1: # 10% —à–∞–Ω—Å –Ω–∞ –æ–ø–µ—á–∞—Ç–∫—É –≤ –∫–æ–Ω—Ü–µ
                    res = res[:-1] + (res[-1] * 2) if res[-1].isalpha() else res
                return res, confidence

        # 2. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞
        math_result = self.solve_math(message)
        if math_result:
            return math_result, 1.0

        # 3. –ò–Ω—Ç–µ–Ω—Ç (–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –∫–∞–∫ –¥–µ–ª–∞ –∏ —Ç.–¥.)
        intent = self.detect_conversational_intent(message)
        if intent:
            if intent == 'greeting':
                state = self.conversation_states.get(user_id)
                if isinstance(state, dict):
                    last_greet = state.get("last_greeting_time")
                    if isinstance(last_greet, datetime):
                        diff = (datetime.now() - last_greet).total_seconds()
                        if diff < 1800: # 30 –º–∏–Ω—É—Ç –∫—É–ª–¥–∞—É–Ω –Ω–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
                            return "", 0.0 

            self.track_topic(user_id, message, intent)
            response = self.get_user_contextual_response(user_id, intent)
            if response:
                return response, 0.95

        # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
        entities = self.extract_entities(message)
        if entities:
            username_display = username or f"User_{user_id}"
            user_name = self.km.get_user_name(user_id) or "–¥—Ä—É–≥"

            if 'name' in entities:
                name = entities['name']
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏–º—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–Ω–æ–≥–æ —Ü–∏—Ñ—Ä
                digit_count = sum(1 for char in name if char.isdigit())
                if len(name) >= 2 and len(name) <= 30 and digit_count <= 2:
                    existing_name = self.km.get_user_name(user_id)
                    # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏–º—è –µ—â–µ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–ª–∏ –µ—Å–ª–∏ —ç—Ç–æ –¥—Ä—É–≥–æ–µ –∏–º—è
                    if not existing_name or existing_name.lower() != name.lower():
                        self.km.save_user_info(user_id, 'name', name, username_display)
                        return f"–ü—Ä–∏—è—Ç–Ω–æ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è, {name}! üëã –ó–∞–ø–æ–º–Ω–∏–ª!", 0.95

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥—Ä—É–≥–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–≥–æ—Ä–æ–¥, —Ä–∞–±–æ—Ç–∞ –∏ —Ç.–¥.)
            for entity_type, value in entities.items():
                if entity_type != 'name':  # –ò–º—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—ã—à–µ
                    self.km.save_user_info(user_id, entity_type, value, username_display)

            if 'age' in entities:
                return f"–û–≥–æ, {entities['age']}! {user_name}, –∫—Ä—É—Ç–æ–π –≤–æ–∑—Ä–∞—Å—Ç! üòä", 0.9
            elif 'city' in entities:
                return f"{entities['city']}? {user_name}, —Å–ª—ã—à–∞–ª, —Ç–∞–º –∫—Ä–∞—Å–∏–≤–æ! üèô", 0.9
            elif 'likes' in entities:
                return f"–ú–º–º, {entities['likes']}... {user_name}, —É —Ç–µ–±—è —Ö–æ—Ä–æ—à–∏–π –≤–∫—É—Å! üòã", 0.9

        # 5. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏ (—Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
        relevant = self.find_relevant_responses(message, user_id)
        if isinstance(relevant, list) and len(relevant) > 0:
            best = relevant[0]
            if isinstance(best, dict) and best.get('similarity', 0) > 0.45:
                user_name = self.km.get_user_name(user_id) or "–°–ª—É—à–∞–π"
                at_user = best.get('user', 'Unknown')
            
                # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–º –∏ –¥–æ–±–∞–≤–∏–º "–ª–∏—á–Ω–æ—Å—Ç–∏"
                templates = [
                    f"–ü–æ–º–Ω—é, @{at_user} –≥–æ–≤–æ—Ä–∏–ª: \"{best['text'][:100]}...\"",
                    f"–ö–∞–∂–µ—Ç—Å—è, @{at_user} —É–∂–µ —É–ø–æ–º–∏–Ω–∞–ª —ç—Ç–æ: \"{best['text'][:100]}...\"",
                    f"{user_name}, –≤–æ—Ç —á—Ç–æ —è –Ω–∞—à–µ–ª –≤ –ø–∞–º—è—Ç–∏ –æ—Ç @{at_user}: \"{best['text'][:100]}...\"",
                    f"–ê–≥–∞! –í—Å–ø–æ–º–Ω–∏–ª —Å–ª–æ–≤–∞ @{at_user}: \"{best['text'][:100]}...\" üßê"
                ]
                return random.choice(templates), 0.7

        # 6. –í–æ–ø—Ä–æ—Å—ã –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ (–ö—Ç–æ —è?)
        user_info = self.km.get_user_info(user_id)
        if user_info:
            if any(p in message_lower for p in ['–∫—Ç–æ —è', '–∫–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç']):
                if 'name' in user_info:
                    return f"–¢—ã - {user_info['name']['value']}! –Ø —Å–≤–æ–∏—Ö –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∑–∞–±—ã–≤–∞—é! üòâ", 1.0
                return "–ú—ã –≤—Ä–æ–¥–µ –∑–Ω–∞–∫–æ–º—ã, –Ω–æ –∏–º—è —è –Ω–µ –∑–∞–ø–∏—Å–∞–ª. –ù–∞–ø–æ–º–Ω–∏—à—å?", 0.8
                
            if any(p in message_lower for p in ['–æ—Ç–∫—É–¥–∞ —è', '–≥–¥–µ —è –∂–∏–≤—É']):
                if 'city' in user_info:
                    return f"–°–ª—É—à–∞–π, —Ç—ã –∂–µ –∏–∑ –≥–æ—Ä–æ–¥–∞ {user_info['city']['value']}! –Ø –≤—Å—ë –ø–æ–º–Ω—é! üèô", 1.0
        
        # 7. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∏–∑–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        return "", 0.0

    def generate_proactive_hook(self, history: List[Dict]) -> Optional[str]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∂–∏–≤–æ–π –∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è –æ–∂–∏–≤–ª–µ–Ω–∏—è –±–µ—Å–µ–¥—ã"""
        if not history:
            return None

        # –°–æ–±–∏—Ä–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        users = list(set([m.get('sender', 'Unknown') for m in history if m['role'] == 'user']))
        if not users:
            return None
        
        target_user = random.choice(users)
        users_list = ", ".join([f"@{u}" for u in users[:3]])
        
        all_text = " ".join([m['content'] for m in history if m['role'] == 'user'])
        tokens = [t for t in self.tokenize(all_text) if t not in self.stop_words]
        
        if not tokens:
            return None

        # –¢–æ–ø –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        freq = {}
        for t in tokens:
            freq[t] = freq.get(t, 0) + 1
        sorted_freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        main_topic = sorted_freq[0][0]

        # –¢–∏–ø—ã —Ö—É–∫–æ–≤
        hook_types = ['question', 'memory', 'pivot', 'short_comment']
        weights = [0.4, 0.2, 0.2, 0.2]
        hook_type = random.choices(hook_types, weights=weights)[0]

        if hook_type == 'question':
            questions = [
                f"–°–ª—É—à–∞–π—Ç–µ, –∞ —á—Ç–æ –≤—ã —Ä–µ–∞–ª—å–Ω–æ –¥—É–º–∞–µ—Ç–µ –ø—Ä–æ {main_topic}? üòâ",
                f"@{target_user}, –∞ –∫–∞–∫ —Ç–µ–±–µ –≤–æ–æ–±—â–µ {main_topic}? –ï—Å—Ç—å –º—ã—Å–ª–∏?",
                f"–ö—Å—Ç–∞—Ç–∏, {main_topic} ‚Äî —ç—Ç–æ –∂–µ —Ä–µ–∞–ª—å–Ω–æ –±–∞–∑–∞ –∏–ª–∏ –∫—Ä–∏–Ω–∂? –ß—Ç–æ —Å–∫–∞–∂–µ—Ç–µ?",
                f"–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø—Ä–æ {main_topic}... –ê –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–∞—à—É –∂–∏–∑–Ω—å? ü§î"
            ]
            return random.choice(questions)

        elif hook_type == 'memory':
            # –ü—ã—Ç–∞–µ–º—Å—è –≤—Å–ø–æ–º–Ω–∏—Ç—å —á—Ç–æ-—Ç–æ —Å–≤—è–∑–∞–Ω–Ω–æ–µ —Å —Ç–æ–ø–∏–∫–æ–º –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            return f"–•–º, {main_topic}... –ü–æ–º–Ω—é, –∫—Ç–æ-—Ç–æ —Ç—É—Ç –æ–±–º–æ–ª–≤–∏–ª—Å—è –ø—Ä–æ —á—Ç–æ-—Ç–æ –ø–æ—Ö–æ–∂–µ–µ. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ? üòâ"

        elif hook_type == 'pivot':
            pivots = [
                "–¢–∞–∫, —Å —ç—Ç–∏–º –ø–æ–Ω—è—Ç–Ω–æ. –ê —á—Ç–æ —Ç–∞–º –ø–æ –ø–ª–∞–Ω–∞–º –Ω–∞ –≤—ã—Ö–∏? üçª",
                "–õ–∞–¥–Ω–µ–Ω—å–∫–æ, –ø—Ä–æ–µ—Ö–∞–ª–∏. –ï—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–∏–Ω—Ç–µ—Ä–µ—Å–Ω–µ–µ? üî•",
                "–ö—Å—Ç–∞—Ç–∏, –∞ –≤—ã –≤–∏–¥–µ–ª–∏, —á—Ç–æ —Å–µ–π—á–∞—Å –≤ —Ç—Ä–µ–Ω–¥–∞—Ö? –û–±—Å—É–¥–∏–º?",
                f"–õ–∞–¥–Ω–æ, {main_topic} ‚Äî —ç—Ç–æ –∫—Ä—É—Ç–æ, –Ω–æ —è —Ç—É—Ç –ø–æ–¥—É–º–∞–ª –æ –¥—Ä—É–≥–æ–º..."
            ]
            return random.choice(pivots)

        else: # short_comment
            sentiments = [self.detect_sentiment(m['content']) for m in history if m['role'] == 'user']
            sent_counts = {s: sentiments.count(s) for s in set(sentiments) if s}
            dominant = max(sent_counts, key=lambda k: sent_counts[k]) if sent_counts else 'neutral'
            
            comments = {
                'joy': [f"–†–∞–¥—É–µ—Ç –≤–∞—à –¥–≤–∏–∂! {users_list}, –≤—ã –æ–≥–æ–Ω—å! üî•", "–ü–æ–∑–∏—Ç–∏–≤–Ω–æ —Ç—É—Ç —É –≤–∞—Å! ‚ú®"],
                'sadness': ["–ß—Ç–æ-—Ç–æ –≥—Ä—É—Å—Ç–Ω–æ —Å—Ç–∞–ª–æ... –†–µ–±—è—Ç, –≤—Å—ë –±—É–¥–µ—Ç –æ–∫! ü´Ç", "–ù–µ –∫–∏—Å–Ω–∏—Ç–µ! üòâ"],
                'anger': ["–£—Ö, –∂–∞—Ä–∞! –ü–æ—Å–ø–æ–∫–æ–π–Ω–µ–µ, —Ä–µ–±—è—Ç. üò§", "–î–∞–≤–∞–π—Ç–µ –±–µ–∑ –∞–≥—Ä–µ—Å—Å–∏–∏, –º–∏—Ä-–¥—Ä—É–∂–±–∞! ‚úåÔ∏è"],
                'neutral': [f"–í–∏–∂—É, {main_topic} –≤–∞—Å –Ω–µ –æ—Ç–ø—É—Å–∫–∞–µ—Ç! üòé", "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ –≤–∞–º–∏..."]
            }
            return random.choice(comments.get(dominant, comments['neutral']))
